<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<!--
 *	$Revision$
-->
<HTML>
<HEAD>
	<META HTTP-EQUIV="CONTENT-TYPE" content="text/html; charset=iso-8859-1">
	<META HTTP-EQUIV="CONTENT-SCRIPT-TYPE" content="text/javascript">
	<META HTTP-EQUIV="CONTENT-STYLE-TYPE" content="text/css">
	<META HTTP-EQUIV="Cache-Control" content="max-age=0">
	<META http-equiv="Cache-Control" content="no-cache">
	<META HTTP-EQUIV="expires" content="0">
	<META HTTP-EQUIV="Expires" CONTENT="Tue, 01 Jan 1980 1:00:00 GMT">
	<META HTTP-EQUIV="Pragma" CONTENT="no-cache">
	<TITLE>Mark's Code Fractal</TITLE>
	<LINK rel="stylesheet" HRef="css/cf-content.css" type="text/css">
</HEAD>
<BODY>
<p>
<CENTER><H1>
<A Target="_blank" HRef="https://msobkow.github.io//?source=website"><img align=vcenter src="resources/MSS-BadgeLogo-98x80.png" width="100" height="80"/></A>
&nbsp;&nbsp;&nbsp;Mark's Code Fractal&nbsp;&nbsp;&nbsp;
</H1></CENTER>
<p>
<table cols="3" width="95%" align="center">
	<tr><td align="left"><A HRef="about.html">Prev</A></td>
		<td align="center"><A HRef="index.html">Home</A></td>
		<td align="right"><A HRef="msscf.html">Next</A></td>
		</tr>
</table>
<p>
<CENTER><H2>Mark's Code Fractal</H2></CENTER>
<p>
Mark's Code Fractal is a tool used to accelerate the production of the repetitive code that comprises 75-85% of the typical enterprise business application - the "glue" between the business logic programmer and the backing data servers and databases.  It does so by applying a knowledge base of General Expansion Rule (GEL) files following specific XML document schemas and semantic constraints to a Business Application Model (BAM), which is at it's heart an enhanced ERD defined as an XML document following yet another schema specification.
<p>
The BAM defines the tables, indexes, and relationships of an ERD, including Superclass, Container/Component, Parent/Child, and Lookup relationships.  It also provides the means of encapsulating customizations of the output source code, though the approach for that will change radically for Mark's Code Fractal 3.1 as opposed to MSS Code Factory 2.13, allowing for much more flexibility than the current hard-coded approach to such customizations.
<p>
The long-term goal is to produce a service interface for an instance of MCF that is deployed on a server cluster to manage and coordinate access to the BAM specifications, allow for team editing of the specifications, and to provide a code production service based on the standard enterprise knowledge base the site has chosen to develop.
<p>
The current 2.13 knowledge base is only a <i>prototype</i> that is intended to be heavily customized and adapted to the unique needs of each enterprise' standards and toolkits.  As long as it takes free-form text input as a specification, you should be able to produce those specification files automatically by customizing your own version of the knowledge base.  My own personal main contribution is the knowledge base; the tool itself is only a tool - it is the <i>knowledge</i> behind the specifications that matter, and the ability to <i>customize</i> and <i>enhance</i> that knowledge and those specifications that is "worth something" to those who have coin to spend.
<p>
Therefore the cfcore, various *bam* packages, and knowledge base are released under a Dual GPLv3/Commercial license while the remainder of the source code is under a Dual LGPLv3/Commercial license so that the vast majority of uses for that code are completely open, and you can build applications using the foundation packages and libraries as-is without issue.
<p>
<H3>So what exists currently?</H3>
<p>
Currently the repositories provide the complete set of source code, models, and rule bases from MSS Code Factory 2.13 under the logical domain "org.msscf", which doesn't actually exist.  The copyright is not based on the domain, however, but on my name and contact information. In the bin-v2_13 repository are a number of scripts which can be used to set up a code factory environment. For example, I have my source trees for mcf and msscode under /opt/mcf and /opt/msscode respectively, with symbolic links from my home directory to those directories. I recommend creating /opt/msscode/v2_13-maven and cloning the various *2_13* repositories under there.
<p>
All of the io.github.msobkow repositories are for Mark's Code Factory 3.1, which is emphatically a work in progress and currently consists of untested and unverified code that mostly clean compiles. Those should be cloned under /opt/mcf/v3_1/java/ (trust me on the directory hierarchy; that's how the output gets laid out by the tool.)
<p>
Yes, it is a lot of repositories, and a lot more are coming over the next few months to deal with message communications channels, RAM storage, a document editor object, and a few other things I have in mind. One major feature, one new repo for each of the three component projects.
<p>
Once you've cloned the repos, you need to set up your environment. For your .profile, you need something like this:
<pre>
<code id="dotProfileText">
export ANT_HOME=/opt/apache-ant-1.10.15
export MAVEN_HOME=/opt/apache-maven-3.9.11
export JFX_HOME=/usr/lib/jvm/javafx-sdk-25.0.1
export PATH_TO_FX=$JFX_HOME/lib

export JAVA_HOME=/usr/lib/jvm/jdk-25.0.1+8
export JDK_HOME=/usr/lib/jvm/jdk-25.0.1+8
export JAVA="$JDK_HOME/bin/java"
export JAVAC="$JDK_HOME/bin/javac"
export NETBEANS_HOME=/usr/lib/apache-netbeans/

export MSSCF_HOME=/opt/msscode
export MSSCFHOME=$MSSCF_HOME
export MSSCF_WORKKB=$MSSCF_HOME/v2_13-maven/org.msscf.msscf.v2_13.cfkbase/src/main/resources
export MCF_HOME=/opt/mcf/v3_1/java
export JSEP=":"
export PSEP=":"
export DSEP="/"

export PATH=/home/msobkow/bin:$MCF_HOME/bin-v3_1:$MSSCF_HOME/v3_1-maven/java/bin-v3_1:$MSSCF_HOME/v2_13-maven/bin-v2_13:$MSSCF_HOME/v2_12/bin:$JDK_HOME/bin:$JDK_HOME/lib:$PATH_TO_FX:$MAVEN_HOME/bin:$MAVEN_HOME/lib:$ANT_HOME/bin:$ANT_HOME/lib:$NETBEANS_HOME/bin:/usr/sbin:$PATH
</code>
</pre>
<p>
Then we have some useful .bash_aliases, though they aren't required, just handy:
<pre>
<code id="bashAliasesText">
alias ls='/usr/bin/ls -AC --classify=auto --group-directories-first '
alias ll='/usr/bin/ls -l '
alias initgit='eval "$(ssh-agent -s)"; ssh-add ~/.ssh/id_ed25519'
alias ollamals="docker exec -it ollama ollama 'ls'"
alias ollama="docker exec -it ollama ollama "
alias cdkb='cd /opt/msscode/v2_13-maven/org.msscf.msscf.v2_13.cfkbase/src/main/resources'
alias cdmsscf='cd /opt/msscode'
alias cdmcf='cd /opt/mcf/v3_1/java'
alias lsd="/usr/bin/ls -A --classify=always . | grep '.*/$';/usr/bin/ls -A --classify=always . | grep '.*@$'"
</code>
</pre>
<p>
Then you set up a .msscfrc configuration file for version 2.13 to use, which has to be located in your home directory:
<p>
<pre>
<code id="dotMsscfrcText">
msscf.cartridgedir=/opt/msscode/v2_13-maven/org.msscf.msscf.v2_13.cfkbase/src/main/resources
#msscf.cartridgedir1=/opt/msscode/v2_13-maven/cfkbase-v2_13/cartridge-2.13
msscf.modeldir=/opt/msscode/v2_13-maven/cfmodel-v2_13/model-2.13
msscf.rootgendir=/opt/mcf/v3_1
</code>
</pre>
<p>
Start a new login shell reading those environmental changes, and within it you should be able to run the utility program "BuildCFAll213.bash", which will run through all the v2.13 packages and build and install them in the correct order, eventually depositing binaries in bin-v2_13. Once you've done that, you're ready to run the tool. As a test, specify "msscf.rootgendir" to point somewhere other than overwriting the /opt/mcf source tree, and run "ManufactureCFSec31Java.bash 42" (the number you pass doesn't matter, it used to be a build number - I may resurrect that.) You should see many messages rapidly streaming by about the files being produced.
<p>
Depending on the speed and storage IO contention on the box you're using, this tool can produce roughly 2.5 million lines of source files in under 5 minutes of coordinated and clean-compiling JPA data IO, Spring service implementation, general buffer based IO, RAM storage of buffers, a structured SAX Parser implementation with associated .xsd files, and the usual supporting cast of pom.xml's, .gitignores, etc.
<p>
Now go forth and create your own models. Although you can't clone and enhance or build on the BAM model without a license or by following GPLv3 restrictions, you <i>can</i> review it for <i>examples</i> of how to model certain data structures and relationships.  If you're later feeling brave, dive into the knowledge base itself and create your own custom rules, potentially for entirely different languages and frameworks. Above all else: Enjoy the Power!
<p>
<H2>Short term plans</H2>
<p>
Among the things I have planned in the near future are:
<p>
<ul>
<li>DONE. A simple test program similar to cfxxxjpatest called cfxxxramtest, which will just make sure the code all wires together successfully.</li>
<li>NEXT UP: Bootstrap code for priming the CFSec database with the "system" cluster, SysCluster record, "system" tenant, and "admin" user for the cluster.  This also requires enhancing ICFSecSchema with static final AtomicReference<CFLibDbKeyHash256> hooks for SysClusterId, SysTenantId, and SysAdminId. The actual values will be different for each cluster, as the keys are randomly generated when th cluster is first initialized.</li>
<li>Rework the old cfxxxsaxramldr mains into Spring Boot applications and testing those; cfxxxramtest will provide the foundation of the updates. This requires a bootstrapped RAM database so the data can be loaded into it.  cfxxxsaxjpaldr mains will follow.</li>
<li>DONE. The base for the DbKeyHash implementations now uses a 64-bit ClusterCode instead of a 32-bit Machine Id.  This change is more than name, but in fundamental meaning, because in the Code Fractal world, a ClusterCode is a randomly generated value stored in the singleton SysCluster table.  This value is randomly generated on system install, and in the future will be submitted to a central server for registration.  But the only way I could automate such processing right now is through the use of my personal email account with one of my ISPs, and I don't think they'd appreciate the potential traffic flood (i.e. Part of setup is sending the registration information encrypted by some known public key so that my email account can decrypt it, verify the ClusterCode is unique and accept the registration, or detect the ClusterCode collision and find a free random one to use instead, and conditionally accept the registration with the proviso that the node change it's ClusterCode to the assigned id.  That way 99% of the time the _client_ produces the ClusterCode values, and the server only has to _register_ them.  The internal keys used for the "system" Cluster and "system" tenant are irrelevant, and will be unique to each cluster, as will the id of the "admin" account for the cluster. No easy "defaults" other than the initial "admin" password being a known string that gets changed on initial login.</li>
<li>Downgrade from JDK25 syntax to JDK17 syntax with the intent of using GWT for Spring for the user interfaces, as I know GWT's commercial relative fairly well.  I have some ideas on how to go about mapping my JavaFX code to GWT code... and getting dynamic layout flow along the way!  It's just fortunate GWT at least supports JDK17; when I was first exposed to it it only supported JDK8 code.</li>
<li>Rework the Obj and Ram layers to use ConcurrentHashMap implementations instead of HashMap implementations.  I need the code to be maximally multi-threaded, and I've learned how to go about doing so on the last major job I worked - I spent about six months deep in the bowels of implementing generic caching with their code base, but what I carried away from it was a deep understanding of <i>how</i> to use ConcurrentHashMaps successfully. Part and parcel of this will be removing the @version tags from the revision attribute of the buffers and JPA objects, and manage those through fractal code to detect collisions and to decide which version of data to keep whenever trying to update the ConcurrentHashMap data records/buffers.</li>
<li>Revisiting the way I bind table and factory implementations to the schemas.  I envision a layered architecture going forward, where layers of schema code can wrap other layers of schema code, such as a generic variation of the Ram storage implementing a generic Cache that can be layered over a transport Buff or database JPA storage back end, which may in turn reference other layers. Right now you can only bind one to the framework, which is pretty much useless for dealing with the very real world situation of wanting a separate <i>session</i> cache for each user session that <i>isn't</i> shared across the code base, but still has to be located in a consistent and predictable fashion. I foresee a plethora of callbacks for hooking it all together the way I want things to function.  Note that only the session cache would have information about the ICFSecAuthorization reference for the user's session - everything else would have to locate the authentication to use via callbacks.</li>
<li>Testing the Ram implementation as much as I can.  I need it stable and reliable before I clone the relevant rules to the engine construction rule set used by CFCore, and start hacking it apart to focus <i>only</i> on the key pieces needed by CFCore, which does <i>not</i> include JPA storage and might not even incorporporate the interface approach I've used in the end, but map everything directly and explicitly to Buff implementations. We'll see; things changed a lot this go-round, and CFCore has some rather <i>specific</i> requirements that don't apply to the generic Ram storage, such as retaining the <i>instance</i> of the rules that are passed in, and returning those instances directly for queries, rather than returning clones of records.</li>
<li>Once the cfengine rule base is up to date for 3.1 and I'm content with the fractal portion of CFCore 3.1, it'll be time to carefully migrate and refresh the custom code aspects of CFCore from 2.13 to 3.1.  There are going to be a few significant changes, some of which I hope improve performance rather substantially.  Only then can I work on refreshing what was the "java+msscf" rules but which will be the "java+mcf" going forward.  Switching all references from MssCF to Mcf is one of the tasks for the CFCore migration.</li>
<li>With CFCore done, it is now possible to produce the mcf layers for the three sub-projects, and to do the manual custom CFBamCustMcf code migrations.  A lot of the existing 2.13 code base won't be migrated, because it is repetitive boilerplate code for output customization that is hardwired to the schema and table objects. Going forward, those will by dynamically defined subobjects of the Schema and Table objects, with their name used in the resolution process by the knowledge base for 3.1 instead of hard-coded verbs. This will require a two-pass approach to initializing the engine before producing output for any models. Compiling the knowledge base provides the engine with the names of the expansion tags/verbs used for customizing the code, whether they are associated with schemas, tables, or both, and thereby providing the name resolution lookup data required to properly validate the models loaded into the system after the rules are primed.</li>
</ul>
</BODY>
</HTML>
