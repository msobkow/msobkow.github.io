<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<!--
 *	$Revision$
-->
<HTML>
<HEAD>
	<META HTTP-EQUIV="CONTENT-TYPE" content="text/html; charset=iso-8859-1">
	<META HTTP-EQUIV="CONTENT-SCRIPT-TYPE" content="text/javascript">
	<META HTTP-EQUIV="CONTENT-STYLE-TYPE" content="text/css">
	<META HTTP-EQUIV="Cache-Control" content="max-age=0">
	<META http-equiv="Cache-Control" content="no-cache">
	<META HTTP-EQUIV="expires" content="0">
	<META HTTP-EQUIV="Expires" CONTENT="Tue, 01 Jan 1980 1:00:00 GMT">
	<META HTTP-EQUIV="Pragma" CONTENT="no-cache">
	<TITLE>Mark's Code Fractal</TITLE>
	<LINK rel="stylesheet" HRef="css/cf-content.css" type="text/css">
</HEAD>
<BODY>
<p>
<CENTER><H1>
<A Target="_blank" HRef="https://msobkow.github.io//?source=website"><img align=vcenter src="resources/MSS-BadgeLogo-98x80.png" width="100" height="80"/></A>
&nbsp;&nbsp;&nbsp;Mark's Code Fractal&nbsp;&nbsp;&nbsp;
</H1></CENTER>
<p>
<table cols="3" width="95%" align="center">
	<tr><td align="left"><A HRef="about.html">Prev</A></td>
		<td align="center"><A HRef="index.html">Home</A></td>
		<td align="right"><A HRef="msscf.html">Next</A></td>
		</tr>
</table>
<p>
<CENTER><H2>Mark's Code Fractal</H2></CENTER>
<p>
Mark's Code Fractal is a tool used to accelerate the production of the repetitive code that comprises 75-85% of the typical enterprise business application - the "glue" between the business logic programmer and the backing data servers and databases.  It does so by applying a knowledge base of General Expansion Rule (GEL) files following specific XML document schemas and semantic constraints to a Business Application Model (BAM), which is at it's heart an enhanced ERD defined as an XML document following yet another schema specification.
<p>
The BAM defines the tables, indexes, and relationships of an ERD, including Superclass, Container/Component, Parent/Child, and Lookup relationships.  It also provides the means of encapsulating customizations of the output source code, though the approach for that will change radically for Mark's Code Fractal 3.1 as opposed to MSS Code Factory 2.13, allowing for much more flexibility than the current hard-coded approach to such customizations.
<p>
The long-term goal is to produce a service interface for an instance of MCF that is deployed on a server cluster to manage and coordinate access to the BAM specifications, allow for team editing of the specifications, and to provide a code production service based on the standard enterprise knowledge base the site has chosen to develop.
<p>
The current 2.13 knowledge base is only a <i>prototype</i> that is intended to be heavily customized and adapted to the unique needs of each enterprise' standards and toolkits.  As long as it takes free-form text input as a specification, you should be able to produce those specification files automatically by customizing your own version of the knowledge base.  My own personal main contribution is the knowledge base; the tool itself is only a tool - it is the <i>knowledge</i> behind the specifications that matter, and the ability to <i>customize</i> and <i>enhance</i> that knowledge and those specifications that is "worth something" to those who have coin to spend.
<p>
Therefore the cfcore, various *bam* packages, and knowledge base are released under a Dual GPLv3/Commercial license while the remainder of the source code is under a Dual LGPLv3/Commercial license so that the vast majority of uses for that code are completely open, and you can build applications using the foundation packages and libraries as-is without issue.
<p>
<H3>So what exists currently?</H3>
<p>
Currently the repositories provide the complete set of source code, models, and rule bases from MSS Code Factory 2.13 under the logical domain "org.msscf", which doesn't actually exist.  The copyright is not based on the domain, however, but on my name and contact information. In the bin-v2_13 repository are a number of scripts which can be used to set up a code factory environment. For example, I have my source trees for mcf and msscode under /opt/mcf and /opt/msscode respectively, with symbolic links from my home directory to those directories. I recommend creating /opt/msscode/v2_13-maven and cloning the various *2_13* repositories under there.
<p>
All of the io.github.msobkow repositories are for Mark's Code Factory 3.1, which is emphatically a work in progress and currently consists of untested and unverified code that mostly clean compiles. Those should be cloned under /opt/mcf/v3_1/java/ (trust me on the directory hierarchy; that's how the output gets laid out by the tool.)
<p>
Yes, it is a lot of repositories, and a lot more are coming over the next few months to deal with message communications channels, RAM storage, a document editor object, and a few other things I have in mind. One major feature, one new repo for each of the three component projects.
<p>
Once you've cloned the repos, you need to set up your environment. For your .profile, you need something like this:
<pre>
<code id="dotProfileText">
export ANT_HOME=/opt/apache-ant-1.10.15
export MAVEN_HOME=/opt/apache-maven-3.9.11
export JFX_HOME=/usr/lib/jvm/javafx-sdk-25.0.1
export PATH_TO_FX=$JFX_HOME/lib

export JAVA_HOME=/usr/lib/jvm/jdk-25.0.1+8
export JDK_HOME=/usr/lib/jvm/jdk-25.0.1+8
export JAVA="$JDK_HOME/bin/java"
export JAVAC="$JDK_HOME/bin/javac"
export NETBEANS_HOME=/usr/lib/apache-netbeans/

export MSSCF_HOME=/opt/msscode
export MSSCFHOME=$MSSCF_HOME
export MSSCF_WORKKB=$MSSCF_HOME/v2_13-maven/org.msscf.msscf.v2_13.cfkbase/src/main/resources
export MCF_HOME=/opt/mcf/v3_1/java
export JSEP=":"
export PSEP=":"
export DSEP="/"

export PATH=/home/msobkow/bin:$MCF_HOME/bin-v3_1:$MSSCF_HOME/v3_1-maven/java/bin-v3_1:$MSSCF_HOME/v2_13-maven/bin-v2_13:$MSSCF_HOME/v2_12/bin:$JDK_HOME/bin:$JDK_HOME/lib:$PATH_TO_FX:$MAVEN_HOME/bin:$MAVEN_HOME/lib:$ANT_HOME/bin:$ANT_HOME/lib:$NETBEANS_HOME/bin:/usr/sbin:$PATH
</code>
</pre>
<p>
Then we have some useful .bash_aliases, though they aren't required, just handy:
<pre>
<code id="bashAliasesText">
alias ls='/usr/bin/ls -AC --classify=auto --group-directories-first '
alias ll='/usr/bin/ls -l '
alias initgit='eval "$(ssh-agent -s)"; ssh-add ~/.ssh/id_ed25519'
alias ollamals="docker exec -it ollama ollama 'ls'"
alias ollama="docker exec -it ollama ollama "
alias cdkb='cd /opt/msscode/v2_13-maven/org.msscf.msscf.v2_13.cfkbase/src/main/resources'
alias cdmsscf='cd /opt/msscode'
alias cdmcf='cd /opt/mcf/v3_1/java'
alias lsd="/usr/bin/ls -A --classify=always . | grep '.*/$';/usr/bin/ls -A --classify=always . | grep '.*@$'"
</code>
</pre>
<p>
Then you set up a .msscfrc configuration file for version 2.13 to use, which has to be located in your home directory:
<p>
<pre>
<code id="dotMsscfrcText">
msscf.cartridgedir=/opt/msscode/v2_13-maven/org.msscf.msscf.v2_13.cfkbase/src/main/resources
#msscf.cartridgedir1=/opt/msscode/v2_13-maven/cfkbase-v2_13/cartridge-2.13
msscf.modeldir=/opt/msscode/v2_13-maven/cfmodel-v2_13/model-2.13
msscf.rootgendir=/opt/mcf/v3_1
</code>
</pre>
<p>
Start a new login shell reading those environmental changes, and within it you should be able to run the utility program "BuildCFAll213.bash", which will run through all the v2.13 packages and build and install them in the correct order, eventually depositing binaries in bin-v2_13. Once you've done that, you're ready to run the tool. As a test, specify "msscf.rootgendir" to point somewhere other than overwriting the /opt/mcf source tree, and run "ManufactureCFSec31Java.bash 42" (the number you pass doesn't matter, it used to be a build number - I may resurrect that.) You should see many messages rapidly streaming by about the files being produced.
<p>
Depending on the speed and storage IO contention on the box you're using, this tool can produce roughly 2.5 million lines of source files in under 5 minutes of coordinated and clean-compiling JPA data IO, Spring service implementation, general buffer based IO, RAM storage of buffers, a structured SAX Parser implementation with associated .xsd files, and the usual supporting cast of pom.xml's, .gitignores, etc.
<p>
Now go forth and create your own models. Although you can't clone and enhance or build on the BAM model without a license or by following GPLv3 restrictions, you <i>can</i> review it for <i>examples</i> of how to model certain data structures and relationships.  If you're later feeling brave, dive into the knowledge base itself and create your own custom rules, potentially for entirely different languages and frameworks. Above all else: Enjoy the Power!
<p>
<H2>Short term plans</H2>
<p>
Among the things I have planned in the near future are:
<p>
<ul>
<li>DONE. A simple test program similar to cfxxxjpatest called cfxxxramtest, which will just make sure the code all wires together successfully.</li>
<li>Eventually, not now. Reworking the old cfxxxsaxramldr mains into Spring Boot applications and testing those.</li>
<li>Probably won't. Possibly resurrecting my old dbtest model and code, though I've lost the test data that used to go with it. It was useful for excercising all possible combinations of nullable/required, value restricted vs. unrestricted data, etc.</li>
<li>PRIORITY ONE: Tweak the CFLibDbKeyHash base class to specify a 64-bit cluster code instead of whatever system or node id is there now.  The rule is that all nodes accessing a given data store and inserting data directly to it instead of going through a proxied connection <i>MUST</i> have unique and stable IP addresses, whether IPv4 or IPv6 so that the host address portion of the hash core distinguishes id's within the domain of the cluster, the cluster code distinguishes keys between clusters, and the remaining attributes try to randomize the remaining data as thoroughly as possible, but in predictable ways within the nodes of the clusters. Simple set theory buried under a secure hash implementation of varying size. The node ids are more from the concept of a tightly managed cluster, where you have control over the fine-grained configuration of every node on your network that ever accesses any resources at all, and can ensure that the node ids are unique. I won't be able to do that until some future date when there is a central server coordinating the registered cluster codes to ensure no one steps on anyone else, but with each cluster initially submitting a randomly generated cluster code that it uses in the interim until it is told otherwise by a response from a central administrator of some kind, whether automated or otherwise.</li>
<li>NEXT UP: Reworking the Obj and Ram layers to use ConcurrentHashMap implementations instead of HashMap implementations.  I need the code to be maximally multi-threaded, and I've learned how to go about doing so on the last major job I worked - I spent about six months deep in the bowels of implementing generic caching with their code base, but what I carried away from it was a deep understanding of <i>how</i> to use ConcurrentHashMaps successfully.</li>
<li>Revisiting the way I bind table and factory implementations to the schemas.  I envision a layered architecture going forward, where layers of schema code can wrap other layers of schema code, such as a generic variation of the Ram storage implementing a generic Cache that can be layered over a transport Buff or database JPA storage back end, which may in turn reference other layers. Right now you can only bind one to the framework, which is pretty much useless for dealing with the very real world situation of wanting a separate <i>session</i> cache for each user session that <i>isn't</i> shared across the code base, but still has to be located in a consistent and predictable fashion. I foresee a plethora of callbacks for hooking it all together the way I want things to function.  Note that only the session cache would have information about the ICFSecAuthorization reference for the user's session - everything else would have to locate the authentication to use via callbacks.</li>
<li>Testing the Ram implementation as much as I can.  I need it stable and reliable before I clone the relevant rules to the engine construction rule set used by CFCore, and start hacking it apart to focus <i>only</i> on the key pieces needed by CFCore, which does <i>not</i> include JPA storage and might not even incorporporate the interface approach I've used in the end, but map everything directly and explicitly to Buff implementations. We'll see; things changed a lot this go-round, and CFCore has some rather <i>specific</i> requirements that don't apply to the generic Ram storage, such as retaining the <i>instance</i> of the rules that are passed in, and returning those instances directly for queries, rather than returning clones of records.</li>
</ul>
</BODY>
</HTML>
