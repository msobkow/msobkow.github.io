<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN">
<!--
 *	$Revision$
-->
<HTML>
<HEAD>
	<META HTTP-EQUIV="CONTENT-TYPE" content="text/html; charset=iso-8859-1">
	<META HTTP-EQUIV="CONTENT-SCRIPT-TYPE" content="text/javascript">
	<META HTTP-EQUIV="CONTENT-STYLE-TYPE" content="text/css">
	<META HTTP-EQUIV="Cache-Control" content="max-age=0">
	<META http-equiv="Cache-Control" content="no-cache">
	<META HTTP-EQUIV="expires" content="0">
	<META HTTP-EQUIV="Expires" CONTENT="Tue, 01 Jan 1980 1:00:00 GMT">
	<META HTTP-EQUIV="Pragma" CONTENT="no-cache">
	<TITLE>Mark's Code Fractal Development Log</TITLE>
	<LINK rel="stylesheet" HRef="css/cf-content.css" type="text/css">
</HEAD>
<BODY>
<p>
<CENTER><H1>
<A Target="_blank" HRef="https://msobkow.github.io//?source=website"><img align=vcenter src="resources/MSS-BadgeLogo-98x80.png" width="100" height="80"/></A>
&nbsp;&nbsp;&nbsp;Mark's Code Fractal&nbsp;&nbsp;&nbsp;
</H1></CENTER>
<p>
<table cols="3" width="95%" align="center">
	<tr><td align="left"><A HRef="mcf.html">Prev</A></td>
		<td align="center"><A HRef="index.html">Home</A></td>
		<td align="right"><A HRef="msscf.html">Next</A></td>
		</tr>
</table>
<p>
<CENTER><H2>Mark's Code Fractal Development Log</H2></CENTER>
<p>
The following items have been finished recently.  At this point, the focus is on getting the core of the Fractal engine up and running on the latest and greatest code base, which is both JDK17 and JDK25 compatible at this point (I believe there are options for JDK17 that have to be turned on for successful compilation of this code base, however - features that were only proposals at the time of JDK17 are relied on. Those features are full default aspects of Java for JDK25. The primary one is the use of static final variables in interfaces.)
<ul>
<li>A simple test program similar called cfxxxramtest, makes sure the code all wires together successfully and that the empty tables can be find-all'ed.</li>
<li>The base for the DbKeyHash implementations now uses a 64-bit ClusterCode instead of a 32-bit Machine Id.  This change is more than name, but in fundamental meaning, because in the Code Fractal world, a ClusterCode is a randomly generated value stored in the singleton SysCluster table.  This value is randomly generated on system install, and in the future will be submitted to a central server for registration.  But the only way I could automate such processing right now is through the use of my personal email account with one of my ISPs, and I don't think they'd appreciate the potential traffic flood (i.e. Part of setup is sending the registration information encrypted by some known public key so that my email account can decrypt it, verify the ClusterCode is unique and accept the registration, or detect the ClusterCode collision and find a free random one to use instead, and conditionally accept the registration with the proviso that the node change it's ClusterCode to the assigned id.  That way 99% of the time the _client_ produces the ClusterCode values, and the server only has to _register_ them.  The internal keys used for the "system" Cluster and "system" tenant are irrelevant, and will be unique to each cluster, as will the id of the "admin" account for the cluster. No easy "defaults" other than the initial "admin" password being a known string that gets changed on initial login.</li>
<li>2026-02-10 - It turned out that no syntax errors cropped when rebuilding with JDK17, even though I thought the use of final statics in interfaces was a JDK25 feature. The downgrade from JDK25 syntax to JDK17 syntax is with the intent of using GWT for Spring for the user interfaces, so I may not be able to continue with the static finals when I'm relying on the GWT Java processor that translates pure JDK17 code to JavaScript in the client.  I have some ideas on how to go about mapping my JavaFX code to GWT code... and getting dynamic layout flow along the way!  It's just fortunate GWT at least supports JDK17; when I was first exposed to it's commercial relative, only JDK8 code was supported.</li>
</ul>
<p>
2026-02-21 Early AM - Keep AspectJ because it should work after all, but I don't actually need to <i>use</i> AspectJ directly, and shouldn't be referencing it directly in my POMs, apparently. Instead, I need to rely on Spring Boot annotations that exist by default but which get runtime AspectJ style initialization on construction via new() operations after the instance is fully created, but before any actual runtime code is used to <i>invoke</i> the object following the new() operation that created it. So by creating a POJO in a separate schemajpahooks package whose classes are intended to be POJOs merely annotated by @Configurable instead of being full @Components. We don't want Spring to create instances and wire a web of them, we want POJO code to specify configuration layers before JPA is initialized and have their @Autowired references resolved on dynamic construction, not on boot layout processing.  Currently there is one massive hooks object for the Schema that references <i>all</i> the Spring objects that are expected to exists for a JPA framework implementation. This ensures that the entire set of objects is guaranteed to have their singletons instantiated when the schema first constructs the originally-null [Schema]JpaHooksSchema member during the first getSchemaHooks() invocation after it's instantiation.  I think what I have to do next is shift the pom.xml references to the spring-aspects dependency to the final main that initializes everything, because the maven processing consolidates all the class loaders into a single .jar and loader as far as I'm aware; I could be wrong.  If I am wrong about that, just specifying spring-aspects was supposed to get the autowiring working, but it doesn't. Obviously I'm missing something...
<p>
2026-02-20 AM - AspectJ isn't going to do what I want! I need to do a post-construction initialization of the @Autowired references in my JPA Schema wiring/hook POJOs because those objects get constructed _before_ Spring initialization takes place. The initialization of those @Autowired references has to happen _after_ Spring is initialized and has resolved all the "normal" Spring objects in the JPA layer.  By default, AspectJ does it's initialization by wiring itself as a "wedge" constructor with the ajc compiler such that the wedge constructor gets invoked instead of the default constructor, invokes the default constructor internally, and then does the autowiring.  That's not what I need at all.  I need to figure out how to do _dynamic_ bean resolution by name and interface type. I know it can be done 'cause it's been done on prior projects; I just need to relearn how Spring does it (besides, if I don't even remember what I'm looking for, it's probably an API that's changed by now, so I'm better off relearning 4.0 information.) Obviously I then need to remove @Autowired from the POJOs and the spring-aspects annotations, refactor those formerly @Autowired members as AtomicReference&lt;Interface|Class&gt;, and wrap their use in getter methods that use dynamic spring resolution mechanisms to locate the bean by name and by type and wire it to the member using ar.get() and ar.set(expected,value) in the getter method to ensure only one setting of the resolution gets kept by this attribute.  Easy-peasy. Probably have it done today, now that a plan is in place.
<p>
2026-02-19 CANCELLED - It turns out that JPA requires binding the POJOs that use @Autowire directives using AspectJ under the hood.  The key is to add the spring-aspects package to your pom to anything making such references. This allows AspectJ to handle the autowiring for beans that have been annotated with @Configurable (you should specify whether to configure by name or by class; in my case virtually all my bindings are by name, but I haven't added that specification yet - I just read about it a few minutes ago. Once you've done that, you specify a bean in your main executable projects that rely on such annotate POJOs to initialize and wire the AspectJ components. Finally, at runtime, your java command has to specify that spring-aspects.jar is to be loaded as well as the *-spring-boot.jar that was produced as the unified application jar; the easiest way to ensure that is done is to wrap all your *-spring-boot.jars with bash scripts that pass along the command line arguments to the java command, but ensure that the command line arguments for launching the application are consistent.  Even applications which don't rely on JPA should do so; this is a low-priority todo and has been for some time, but this recent change has made it important to take care of as the final step of the AspectJ wiring.
<p>
2026-02-13:14 - ON HOLD - Requires new approach based on 2026-02-20 AM notes - Bootstrap code for priming the CFSec database with the "system" cluster, "system" tenant, "sysadmin" user with initial password "ChangeOnInstall", the SysCluster record, and the SecSession for the bootstrapped security data has been added to the Buff and JPA implementations, with a "bootstrapSchema()" method added to the schema interfaces.  Atomic references for the SysClusterId, SysTenantId, and SysAdminId have been added and are initialized by the security portion of the bootstrapSchema() implementations for CFSecBuffSchema and CFSecJpaSchema.  This is very much a work in progress - now that I've got the atomic references and their accessors, I can code initial security constraints for the table accessor methods that give the admin user global access, and pass in a security object during the bootstrap process that is based on those values.  I want to get the initial security framework wired back in at this point in time as well, so that I've dealt with the special cases needed for the bootstrap processing. The SysCluster record and the actual system Cluster have to be readable by anybody, but trying to read any <i>other</i> Cluster should be secured.  The <i>existence</i> of valid user ids has to be verifiable, but the SecUser table itself shouldn't be globally readable.  SecSession should be tightly restricted, such that only the SysAdmin user can access it other than in-security-process code.  The exceptions that were being thrown by CFSecRamTest have been cleaned up, but there are still two CFSecSecUser entries where there should only be one.
<p>
At this point in time there are some errors in the Ram layer version that lead testing of the Buff implementation to have _two_ SecUser records where I expect _one_, and that is much hairier code to debug than JPA which relies on infrastructure code to do the heavy lifting, so I'm focusing on finishing the JPA code before I try to fix the Ram layer. Infrastructure code that has been already proven to work by many tens of thousands of sites. As the other aspects of the Buff and Ram implementation seem ok, I've cloned the Buff code to the JPA layer, replaced "Buff" with "Jpa", and refactored a bootstrapSecurity() transactional method in a SchemaService bean to do the bootstrap processing as an atomic transaction, as you should either be initialized or uninitialized atomically.
<p>
In the near future after the bootstrap code is in both the cfxxxjpatest and cfxxxramtest code and has been used to successfully bootstrap data for those implementations, the following items are on the "todo" list for the relatively near future. Don't worry, there is plenty more coming after that. I don't expect to be done coding my vision until the end of 2026Q3 at earliest, probably well into 2027Q1.
<ul>
<li>Wire the SAX Parsers to the basic schema interfaces, such that one can pass a file, URI, or text string to schema methods that will apply the wired parser to the file to load it into the system. Modify the bootstrap processing to search out and load any resource files in the bootstrapdata folder under src/main/resources as specified in the bootstrapdata.catalogue resource file located there. Use a method to wire the SAX Parsers to the schema as an appropriate CFLib Xml package interface or base class. Modify the RamTest and JpaTest applications to initialize the SAX Parsers of the backing schemas accordingly.</li>
<li>Rework the old cfxxxsaxramldr mains into Spring Boot applications and testing those; cfxxxramtest will provide the foundation of the updates. This requires a bootstrapped RAM database so the data can be loaded into it.  cfxxxsaxjpaldr mains will follow. Test them reasonably well to verify the Buff/RAM implementation is sane and that the JPA implementation works as well (less likely to have problems with that due to simplified layering compared to the fiendishly huge table objects for RAM.)</li>
<li>Create bootstrap data for the lookup tables of the schemas, especially CFSec, which has the ISO lookups. Verify that it loads and searches properly; there should be a lot of data reported by cfsecramtest and cfsecjpatest once this data is complete. The provisioning of initialization files has to be generalized such that a known resource is retrieved that specifies the initialization data resources in their required load order.</li>
<li>Testing the Ram implementation with the existing HashMaps as much as I can.  I need it stable and reliable before I start working on the ConcurrentHashMap enhancements. By the time the bootstrap data is loaded, the existing Ram implementation will have been exercised for Create, Read, Update, and possibly Delete operations (I forget exactly how some of my plumbing hooks together - it's a lot of years of coding to keep straight in one head, so even I have to check the code to see how it <i>actually</i> works sometimes.)</li>
<li>Rework the Obj and Ram layers to use ConcurrentHashMap implementations instead of HashMap implementations.  I need the code to be maximally multi-threaded, and I've learned how to go about doing so on the last major job I worked - I spent about six months deep in the bowels of implementing generic caching with their code base, but what I carried away from it was a deep understanding of <i>how</i> to use ConcurrentHashMaps successfully. Part and parcel of this will be removing the @version tags from the revision attribute of the buffers and JPA objects, and manage those through fractal code to detect collisions and to decide which version of data to keep whenever trying to update the ConcurrentHashMap data records/buffers.</li>
<li>Testing the Ram implementation with the ConcurrentHashMaps as much as I can.  I need it stable and reliable before I clone the relevant rules to the engine construction rule set used by CFCore.  Again, the bootstrapping of the schemas should sufficiently excercise single-user use of the multi-threaded code base, but I'll have to get creative to come up with a way to <i>concurrently</i> load data; maybe specify the bootstrap files as load groups that can be loaded in parallel to speed the process, with some sort of "requires load group" specifications to weave a dependency hierarchy of them.</li>
<li>Clone the java rulebase so far for the mcf engine rulebase, remembering to rebrand it "mcf" instead of "msscf" as with the old code base. Tear it apart to focus <i>only</i> on the key pieces needed by CFCore, which does <i>not</i> include JPA storage and might not even incorporate the interface approach I've used in the end, but map everything directly and explicitly to Buff implementations. We'll see; things changed a lot this go-round, and CFCore has some rather <i>specific</i> requirements that don't apply to the generic Ram storage, such as retaining the <i>instance</i> of the rules that are passed in, and returning those instances directly for queries, rather than returning clones of records.</li>
<li>Revisiting the way I bind table and factory implementations to the schemas.  I envision a layered architecture going forward, where layers of schema code can wrap other layers of schema code, such as a generic variation of the Ram storage implementing a generic Cache that can be layered over a transport Buff or database JPA storage back end, which may in turn reference other layers. Right now you can only bind one to the framework, which is pretty much useless for dealing with the very real world situation of wanting a separate <i>session</i> cache for each user session that <i>isn't</i> shared across the code base, but still has to be located in a consistent and predictable fashion. I foresee a plethora of callbacks for hooking it all together the way I want things to function.  Note that only the session cache would have information about the ICFSecAuthorization reference for the user's session - everything else would have to locate the authentication to use via callbacks.</li>
<li>Once the cfengine rule base is up to date for 3.1 and I'm content with the fractal portion of CFCore 3.1, it'll be time to carefully migrate and refresh the custom code aspects of CFCore from 2.13 to 3.1.  There are going to be a few significant changes, some of which I hope improve performance rather substantially.  Only then can I work on refreshing what was the "java+msscf" rules but which will be the "java+mcf" going forward.  Switching all references from MssCF to Mcf is one of the tasks for the CFCore migration.</li>
<li>With CFCore done, it is now possible to produce the mcf layers for the three sub-projects, and to do the manual custom CFBamCustMcf code migrations.  A lot of the existing 2.13 code base won't be migrated, because it is repetitive boilerplate code for output customization that is hardwired to the schema and table objects. Going forward, those will by dynamically defined subobjects of the Schema and Table objects, with their name used in the resolution process by the knowledge base for 3.1 instead of hard-coded verbs. This will require a two-pass approach to initializing the engine before producing output for any models. Compiling the knowledge base provides the engine with the names of the expansion tags/verbs used for customizing the code, whether they are associated with schemas, tables, or both, and thereby providing the name resolution lookup data required to properly validate the models loaded into the system after the rules are primed.</li>
</ul>
</BODY>
</HTML>
